{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Full Code.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMrIhnQu+8q5QhYX2YA+0MD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adsamardeep/Twitter-Sentiment-Analysis/blob/master/Full_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnucSwctKF2p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "73b19869-43bb-43d0-9664-da364ffcb1f8"
      },
      "source": [
        "import pandas as pd\n",
        "from termcolor import colored\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define variables\n",
        "COLUMNS = ['id', 'original_text',\t'lang',\t'retweet_count',\t'original_author',\t'sentiment_class']\n",
        "\n",
        "# Read dataset\n",
        "dataset = pd.read_csv('data/dataset.csv', encoding = 'latin-1')\n",
        "print(colored(\"Columns: {}\".format(', '.join(COLUMNS)), \"yellow\"))\n",
        "\n",
        "# Remove extra columns\n",
        "print(colored(\"Useful columns: sentiment_class and original_text\", \"yellow\"))\n",
        "print(colored(\"Removing other columns\", \"red\"))\n",
        "dataset.drop(['id',\t'lang',\t'retweet_count',\t'original_author'], axis = 1, inplace = True)\n",
        "print(colored(\"Columns removed\", \"red\"))\n",
        "\n",
        "# Train test split\n",
        "print(colored(\"Splitting train and test dataset into 80:20\", \"yellow\"))\n",
        "X_train, X_test, y_train, y_test = train_test_split(dataset['original_text'], dataset['sentiment_class'], test_size = 0.20, random_state = 100)\n",
        "train_dataset = pd.DataFrame({\n",
        "\t'Tweet': X_train,\n",
        "\t'Sentiment': y_train\n",
        "\t})\n",
        "print(colored(\"Train data distribution:\", \"yellow\"))\n",
        "print(train_dataset['Sentiment'].value_counts())\n",
        "test_dataset = pd.DataFrame({\n",
        "\t'Tweet': X_test,\n",
        "\t'Sentiment': y_test\n",
        "\t})\n",
        "print(colored(\"Test data distribution:\", \"yellow\"))\n",
        "print(test_dataset['Sentiment'].value_counts())\n",
        "print(colored(\"Split complete\", \"yellow\"))\n",
        "\n",
        "# Save train data\n",
        "print(colored(\"Saving train data\", \"yellow\"))\n",
        "\n",
        "train_dataset.to_csv('data/train.csv', index = False)\n",
        "print(colored(\"Train data saved to data/train.csv\", \"green\"))\n",
        "\n",
        "# Save test data\n",
        "print(colored(\"Saving test data\", \"yellow\"))\n",
        "test_dataset.to_csv('data/test.csv', index = False)\n",
        "print(colored(\"Test data saved to data/test.csv\", \"green\"))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[33mColumns: id, original_text, lang, retweet_count, original_author, sentiment_class\u001b[0m\n",
            "\u001b[33mUseful columns: sentiment_class and original_text\u001b[0m\n",
            "\u001b[31mRemoving other columns\u001b[0m\n",
            "\u001b[31mColumns removed\u001b[0m\n",
            "\u001b[33mSplitting train and test dataset into 80:20\u001b[0m\n",
            "\u001b[33mTrain data distribution:\u001b[0m\n",
            " 0    1371\n",
            " 1     618\n",
            "-1     599\n",
            "Name: Sentiment, dtype: int64\n",
            "\u001b[33mTest data distribution:\u001b[0m\n",
            " 0    330\n",
            "-1    170\n",
            " 1    147\n",
            "Name: Sentiment, dtype: int64\n",
            "\u001b[33mSplit complete\u001b[0m\n",
            "\u001b[33mSaving train data\u001b[0m\n",
            "\u001b[32mTrain data saved to data/train.csv\u001b[0m\n",
            "\u001b[33mSaving test data\u001b[0m\n",
            "\u001b[32mTest data saved to data/test.csv\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAw3FLXUL2gl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "outputId": "06d7af40-6b99-44c3-f50f-50757ae4aa21"
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from termcolor import colored\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Import datasets\n",
        "print(\"Loading data\")\n",
        "train_data = pd.read_csv('data/train.csv')\n",
        "test_data = pd.read_csv('data/test.csv')\n",
        "\n",
        "# Setting stopwords\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "STOPWORDS.remove(\"not\")\n",
        "\n",
        "# Function to expand tweet\n",
        "def expand_tweet(tweet):\n",
        "\texpanded_tweet = []\n",
        "\tfor word in tweet:\n",
        "\t\tif re.search(\"n't\", word):\n",
        "\t\t\texpanded_tweet.append(word.split(\"n't\")[0])\n",
        "\t\t\texpanded_tweet.append(\"not\")\n",
        "\t\telse:\n",
        "\t\t\texpanded_tweet.append(word)\n",
        "\treturn expanded_tweet\n",
        "\n",
        "# Function to process tweets\n",
        "def clean_tweet(data, wordNetLemmatizer, porterStemmer):\n",
        "\tdata['Clean_tweet'] = data['Tweet']\n",
        "\tprint(colored(\"Removing user handles starting with @\", \"yellow\"))\n",
        "\tdata['Clean_tweet'] = data['Clean_tweet'].str.replace(\"@[\\w]*\",\"\")\n",
        "\tprint(colored(\"Removing numbers and special characters\", \"yellow\"))\n",
        "\tdata['Clean_tweet'] = data['Clean_tweet'].str.replace(\"[^a-zA-Z' ]\",\"\")\n",
        "\tprint(colored(\"Removing urls\", \"yellow\"))\n",
        "\tdata['Clean_tweet'] = data['Clean_tweet'].replace(re.compile(r\"((www\\.[^\\s]+)|(https?://[^\\s]+))\"), \"\")\n",
        "\tprint(colored(\"Removing single characters\", \"yellow\"))\n",
        "\tdata['Clean_tweet'] = data['Clean_tweet'].replace(re.compile(r\"(^| ).( |$)\"), \" \")\n",
        "\tprint(colored(\"Tokenizing\", \"yellow\"))\n",
        "\tdata['Clean_tweet'] = data['Clean_tweet'].str.split()\n",
        "\tprint(colored(\"Removing stopwords\", \"yellow\"))\n",
        "\tdata['Clean_tweet'] = data['Clean_tweet'].apply(lambda tweet: [word for word in tweet if word not in STOPWORDS])\n",
        "\tprint(colored(\"Expanding not words\", \"yellow\"))\n",
        "\tdata['Clean_tweet'] = data['Clean_tweet'].apply(lambda tweet: expand_tweet(tweet))\n",
        "\tprint(colored(\"Lemmatizing the words\", \"yellow\"))\n",
        "\tdata['Clean_tweet'] = data['Clean_tweet'].apply(lambda tweet: [wordNetLemmatizer.lemmatize(word) for word in tweet])\n",
        "\tprint(colored(\"Stemming the words\", \"yellow\"))\n",
        "\tdata['Clean_tweet'] = data['Clean_tweet'].apply(lambda tweet: [porterStemmer.stem(word) for word in tweet])\n",
        "\tprint(colored(\"Combining words back to tweets\", \"yellow\"))\n",
        "\tdata['Clean_tweet'] = data['Clean_tweet'].apply(lambda tweet: ' '.join(tweet))\n",
        "\treturn data\n",
        "\n",
        "# Define processing methods\n",
        "wordNetLemmatizer = WordNetLemmatizer()\n",
        "porterStemmer = PorterStemmer()\n",
        "\n",
        "# Pre-processing the tweets\n",
        "print(colored(\"Processing train data\", \"green\"))\n",
        "train_data = clean_tweet(train_data, wordNetLemmatizer, porterStemmer)\n",
        "train_data.to_csv('data/clean_train.csv', index = False)\n",
        "print(colored(\"Train data processed and saved to data/clean_train.csv\", \"green\"))\n",
        "print(colored(\"Processing test data\", \"green\"))\n",
        "test_data = clean_tweet(test_data, wordNetLemmatizer, porterStemmer)\n",
        "test_data.to_csv('data/clean_test.csv', index = False)\n",
        "print(colored(\"Test data processed and saved to data/clean_test.csv\", \"green\"))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "Loading data\n",
            "\u001b[32mProcessing train data\u001b[0m\n",
            "\u001b[33mRemoving user handles starting with @\u001b[0m\n",
            "\u001b[33mRemoving numbers and special characters\u001b[0m\n",
            "\u001b[33mRemoving urls\u001b[0m\n",
            "\u001b[33mRemoving single characters\u001b[0m\n",
            "\u001b[33mTokenizing\u001b[0m\n",
            "\u001b[33mRemoving stopwords\u001b[0m\n",
            "\u001b[33mExpanding not words\u001b[0m\n",
            "\u001b[33mLemmatizing the words\u001b[0m\n",
            "\u001b[33mStemming the words\u001b[0m\n",
            "\u001b[33mCombining words back to tweets\u001b[0m\n",
            "\u001b[32mTrain data processed and saved to data/clean_train.csv\u001b[0m\n",
            "\u001b[32mProcessing test data\u001b[0m\n",
            "\u001b[33mRemoving user handles starting with @\u001b[0m\n",
            "\u001b[33mRemoving numbers and special characters\u001b[0m\n",
            "\u001b[33mRemoving urls\u001b[0m\n",
            "\u001b[33mRemoving single characters\u001b[0m\n",
            "\u001b[33mTokenizing\u001b[0m\n",
            "\u001b[33mRemoving stopwords\u001b[0m\n",
            "\u001b[33mExpanding not words\u001b[0m\n",
            "\u001b[33mLemmatizing the words\u001b[0m\n",
            "\u001b[33mStemming the words\u001b[0m\n",
            "\u001b[33mCombining words back to tweets\u001b[0m\n",
            "\u001b[32mTest data processed and saved to data/clean_test.csv\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brXpG3VPMRYK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 870
        },
        "outputId": "98d925e7-ad19-4220-816d-0d979509f130"
      },
      "source": [
        "import os\n",
        "import tensorflow\n",
        "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "import pandas as pd\n",
        "from termcolor import colored\n",
        "\n",
        "# Load data\n",
        "print(colored(\"Loading train and test data\", \"yellow\"))\n",
        "train_data = pd.read_csv('data/clean_train.csv')\n",
        "test_data = pd.read_csv('data/clean_test.csv')\n",
        "print(colored(\"Data loaded\", \"yellow\"))\n",
        "\n",
        "# Tokenization\n",
        "print(colored(\"Tokenizing and padding data\", \"yellow\"))\n",
        "tokenizer = Tokenizer(num_words = 2000, split = ' ')\n",
        "tokenizer.fit_on_texts(train_data['Clean_tweet'].astype(str).values)\n",
        "train_tweets = tokenizer.texts_to_sequences(train_data['Clean_tweet'].astype(str).values)\n",
        "max_len = max([len(i) for i in train_tweets])\n",
        "train_tweets = pad_sequences(train_tweets, maxlen = max_len)\n",
        "test_tweets = tokenizer.texts_to_sequences(test_data['Clean_tweet'].astype(str).values)\n",
        "test_tweets = pad_sequences(test_tweets, maxlen = max_len)\n",
        "print(colored(\"Tokenizing and padding complete\", \"yellow\"))\n",
        "\n",
        "# Building the model\n",
        "print(colored(\"Creating the LSTM model\", \"yellow\"))\n",
        "model = Sequential()\n",
        "model.add(Embedding(2000, 128, input_length = train_tweets.shape[1]))\n",
        "model.add(SpatialDropout1D(0.4))\n",
        "model.add(LSTM(256, dropout = 0.2))\n",
        "model.add(Dense(3, activation = 'softmax'))\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "# Training the model\n",
        "print(colored(\"Training the LSTM model\", \"green\"))\n",
        "history = model.fit(train_tweets, pd.get_dummies(train_data['Sentiment']).values, epochs = 10, batch_size = 128, validation_split = 0.2)\n",
        "print(colored(history, \"green\"))\n",
        "\n",
        "# Testing the model\n",
        "print(colored(\"Testing the LSTM model\", \"green\"))\n",
        "score, accuracy = model.evaluate(test_tweets, pd.get_dummies(test_data['Sentiment']).values, batch_size = 128)\n",
        "print(\"Test accuracy: {}\".format(accuracy))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[33mLoading train and test data\u001b[0m\n",
            "\u001b[33mData loaded\u001b[0m\n",
            "\u001b[33mTokenizing and padding data\u001b[0m\n",
            "\u001b[33mTokenizing and padding complete\u001b[0m\n",
            "\u001b[33mCreating the LSTM model\u001b[0m\n",
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_5 (Embedding)      (None, 50, 128)           256000    \n",
            "_________________________________________________________________\n",
            "spatial_dropout1d_5 (Spatial (None, 50, 128)           0         \n",
            "_________________________________________________________________\n",
            "lstm_5 (LSTM)                (None, 256)               394240    \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 3)                 771       \n",
            "=================================================================\n",
            "Total params: 651,011\n",
            "Trainable params: 651,011\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "\u001b[32mTraining the LSTM model\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 2070 samples, validate on 518 samples\n",
            "Epoch 1/10\n",
            "2070/2070 [==============================] - 9s 4ms/step - loss: 1.0326 - accuracy: 0.5232 - val_loss: 1.0404 - val_accuracy: 0.4981\n",
            "Epoch 2/10\n",
            "2070/2070 [==============================] - 9s 4ms/step - loss: 1.0057 - accuracy: 0.5377 - val_loss: 1.0464 - val_accuracy: 0.4981\n",
            "Epoch 3/10\n",
            "2070/2070 [==============================] - 9s 4ms/step - loss: 0.9910 - accuracy: 0.5377 - val_loss: 1.0787 - val_accuracy: 0.4981\n",
            "Epoch 4/10\n",
            "2070/2070 [==============================] - 9s 4ms/step - loss: 0.9529 - accuracy: 0.5425 - val_loss: 1.0690 - val_accuracy: 0.4826\n",
            "Epoch 5/10\n",
            "2070/2070 [==============================] - 9s 4ms/step - loss: 0.8944 - accuracy: 0.6164 - val_loss: 1.1281 - val_accuracy: 0.4633\n",
            "Epoch 6/10\n",
            "2070/2070 [==============================] - 9s 4ms/step - loss: 0.8254 - accuracy: 0.6382 - val_loss: 1.1432 - val_accuracy: 0.4131\n",
            "Epoch 7/10\n",
            "2070/2070 [==============================] - 9s 4ms/step - loss: 0.7351 - accuracy: 0.6816 - val_loss: 1.2646 - val_accuracy: 0.4247\n",
            "Epoch 8/10\n",
            "2070/2070 [==============================] - 9s 4ms/step - loss: 0.6424 - accuracy: 0.7348 - val_loss: 1.3767 - val_accuracy: 0.3784\n",
            "Epoch 9/10\n",
            "2070/2070 [==============================] - 9s 4ms/step - loss: 0.5702 - accuracy: 0.7734 - val_loss: 1.4291 - val_accuracy: 0.4093\n",
            "Epoch 10/10\n",
            "2070/2070 [==============================] - 9s 4ms/step - loss: 0.4863 - accuracy: 0.8072 - val_loss: 1.6624 - val_accuracy: 0.4073\n",
            "\u001b[32m<keras.callbacks.callbacks.History object at 0x7faeaa484ef0>\u001b[0m\n",
            "\u001b[32mTesting the LSTM model\u001b[0m\n",
            "647/647 [==============================] - 1s 1ms/step\n",
            "Test accuracy: 0.3956723213195801\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJbJctwmTY-w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "9aaf7ef7-286e-456a-f177-e9a2673ca99a"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from termcolor import colored\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Load data\n",
        "print(colored(\"Loading train and test data\", \"yellow\"))\n",
        "train_data = pd.read_csv('data/clean_train.csv')\n",
        "test_data = pd.read_csv('data/clean_test.csv')\n",
        "print(colored(\"Data loaded\", \"yellow\"))\n",
        "\n",
        "# Tf-IDF\n",
        "print(colored(\"Applying TF-IDF transformation\", \"yellow\"))\n",
        "tfidfVectorizer = TfidfVectorizer(min_df = 5, max_features = 1000)\n",
        "tfidfVectorizer.fit(train_data['Clean_tweet'].apply(lambda x: np.str_(x)))\n",
        "train_tweet_vector = tfidfVectorizer.transform(train_data['Clean_tweet'].apply(lambda x: np.str_(x)))\n",
        "test_tweet_vector = tfidfVectorizer.transform(test_data['Clean_tweet'].apply(lambda x: np.str_(x)))\n",
        "\n",
        "# Training\n",
        "print(colored(\"Training Random Forest Classifier\", \"yellow\"))\n",
        "randomForestClassifier = RandomForestClassifier()\n",
        "randomForestClassifier.fit(train_tweet_vector, train_data['Sentiment'])\n",
        "\n",
        "# Prediction\n",
        "print(colored(\"Predicting on train data\", \"yellow\"))\n",
        "prediction = randomForestClassifier.predict(train_tweet_vector)\n",
        "print(colored(\"Training accuracy: {}%\".format(accuracy_score(train_data['Sentiment'], prediction)*100), \"green\"))\n",
        "\n",
        "print(colored(\"Predicting on test data\", \"yellow\"))\n",
        "prediction = randomForestClassifier.predict(test_tweet_vector)\n",
        "print(colored(\"Testing accuracy: {}%\".format(accuracy_score(test_data['Sentiment'], prediction)*100), \"green\"))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[33mLoading train and test data\u001b[0m\n",
            "\u001b[33mData loaded\u001b[0m\n",
            "\u001b[33mApplying TF-IDF transformation\u001b[0m\n",
            "\u001b[33mTraining Random Forest Classifier\u001b[0m\n",
            "\u001b[33mPredicting on train data\u001b[0m\n",
            "\u001b[32mTraining accuracy: 99.34312210200927%\u001b[0m\n",
            "\u001b[33mPredicting on test data\u001b[0m\n",
            "\u001b[32mTesting accuracy: 49.92272024729521%\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5FydlEQW93K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}